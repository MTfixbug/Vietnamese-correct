{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /home/phuc/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2o9jAybBvouVDCenbS5uYTwSLec_2UxnHYCMYS3gJvZQJ88HZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 01:10:28.937535: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 01:10:29.026528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730398229.059573  208210 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730398229.069888  208210 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 01:10:29.146640: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/phuc/Data_in_VS_code/finalproject/tutorial-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "I0000 00:00:1730398234.104549  208210 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2181 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://a350-171-252-155-167.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [208210]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "I0000 00:00:1730398275.941711  208782 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n",
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     171.252.155.167:0 - \"POST /correct_sentence HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import các thư viện\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import pipeline\n",
    "import string\n",
    "import torch\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import tensorflow as tf\n",
    "\n",
    "# Thiết lập Nest Asyncio để có thể chạy FastAPI trong notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Khởi tạo ứng dụng FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Đường dẫn tới mô hình\n",
    "model1_path = \"/home/phuc/Data_in_VS_code/finalproject/tutorial-env/model/model1.keras\"\n",
    "model2_path = \"/home/phuc/Data_in_VS_code/finalproject/tutorial-env/model/model2\"\n",
    "\n",
    "# Cấu hình TensorFlow để sử dụng GPU nếu có\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Không thể thiết lập set_memory_growth: {e}\")\n",
    "\n",
    "# Tải mô hình Keras\n",
    "model = load_model(model1_path)\n",
    "model.make_predict_function()\n",
    "\n",
    "# Cấu hình Transformers sử dụng GPU nếu có\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "corrector = pipeline(\"text2text-generation\", model=model2_path, tokenizer=model2_path, device=device)\n",
    "\n",
    "# Các biến\n",
    "NGRAM = 2\n",
    "MAXLEN = 40\n",
    "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
    "accepted_char = list(string.digits + ''.join(alphabet))\n",
    "\n",
    "# Các hàm trợ giúp\n",
    "def encoder_data(text, maxlen=MAXLEN):\n",
    "    x = np.zeros((maxlen, len(alphabet)))\n",
    "    for i, c in enumerate(text[:maxlen]):\n",
    "        if c in alphabet:\n",
    "            x[i, alphabet.index(c)] = 1\n",
    "    if i < maxlen - 1:\n",
    "        for j in range(i + 1, maxlen):\n",
    "            x[j, 0] = 1\n",
    "    return x\n",
    "\n",
    "def decoder_data(x):\n",
    "    x = x.argmax(axis=-1)\n",
    "    return ''.join(alphabet[i] for i in x)\n",
    "\n",
    "def batch_predict(ngrams_batch):\n",
    "    batch_input = np.array([encoder_data(' '.join(ngram)) for ngram in ngrams_batch])\n",
    "    preds = model.predict(batch_input, verbose=0)\n",
    "    return [decoder_data(pred).strip('\\x00') for pred in preds]\n",
    "\n",
    "# Hàm chính để sửa lỗi chính tả\n",
    "def correct_sentence(sentence):\n",
    "    def separate_words(text):\n",
    "        return re.sub(r'(?<!^)(?=[A-Z])', ' ', text)\n",
    "\n",
    "    def nltk_ngrams(words, n=2):\n",
    "        return ngrams(words.split(), n)\n",
    "    sentence = sentence.lower()  \n",
    "\n",
    "    sentence = separate_words(sentence)\n",
    "    for i in sentence:\n",
    "        if i not in accepted_char:\n",
    "            sentence = sentence.replace(i, \" \")\n",
    "\n",
    "    ngrams_list = list(nltk_ngrams(sentence, n=NGRAM))\n",
    "    guessed_ngrams = batch_predict(ngrams_list)\n",
    "\n",
    "    candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
    "    for nid, ngram in enumerate(guessed_ngrams):\n",
    "        for wid, word in enumerate(re.split(' +', ngram)):\n",
    "            candidates[nid + wid].update([word])\n",
    "\n",
    "    if not candidates or all(len(c) == 0 for c in candidates):\n",
    "        return \"Không có từ nào để sửa.\"\n",
    "\n",
    "    first_guess = ' '.join(c.most_common(1)[0][0] for c in candidates if c)\n",
    "\n",
    "    # Chạy mô hình transformers\n",
    "    predictions = corrector(first_guess, max_length=512)\n",
    "    final_output = predictions[0]['generated_text']\n",
    "    return final_output\n",
    "\n",
    "# Định nghĩa cấu trúc request và response\n",
    "class SentenceRequest(BaseModel):\n",
    "    sentence: str\n",
    "\n",
    "class SentenceResponse(BaseModel):\n",
    "    corrected_sentence: str\n",
    "\n",
    "# Định nghĩa route cho API\n",
    "@app.post(\"/correct_sentence\", response_model=SentenceResponse)\n",
    "async def api_correct_sentence(request: SentenceRequest):\n",
    "    corrected = correct_sentence(request.sentence)\n",
    "    return SentenceResponse(corrected_sentence=corrected)\n",
    "\n",
    "# Khởi tạo ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"Public URL: {public_url}\")\n",
    "\n",
    "# Chạy ứng dụng FastAPI với Uvicorn\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

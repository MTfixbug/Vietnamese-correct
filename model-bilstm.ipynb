{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9753425,"sourceType":"datasetVersion","datasetId":5971342}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pickle\nimport os\n!pip install unidecode\nfrom unidecode import unidecode\nimport itertools\nfrom nltk import ngrams\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-29T11:13:38.316504Z","iopub.execute_input":"2024-10-29T11:13:38.317342Z","iopub.status.idle":"2024-10-29T11:13:49.884499Z","shell.execute_reply.started":"2024-10-29T11:13:38.317298Z","shell.execute_reply":"2024-10-29T11:13:49.883268Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: unidecode in /opt/conda/lib/python3.10/site-packages (1.3.8)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# Bước 1: Tải file\nwith open('/kaggle/input/trainplusdataset/combined_file.pkl', 'rb') as file:\n    data = pickle.load(file)\n\n# Bước 2: Chuyển đổi tất cả các chuỗi thành chữ in hoa\nif isinstance(data, list):  # Nếu dữ liệu là danh sách\n    data = [item.upper() if isinstance(item, str) else item for item in data]\nelif isinstance(data, dict):  # Nếu dữ liệu là từ điển\n    data = {key: value.upper() if isinstance(value, str) else value for key, value in data.items()}\n\n# Bước 3: Lưu lại dữ liệu đã chuyển đổi vào file mới\nwith open('/kaggle/working/combined_file_upper.pkl', 'wb') as file:\n    pickle.dump(data, file)\n\nprint(\"Đã chuyển đổi thành công và lưu vào combined_file_upper.pkl\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:52:36.759583Z","iopub.execute_input":"2024-10-29T10:52:36.760062Z","iopub.status.idle":"2024-10-29T10:53:09.312880Z","shell.execute_reply.started":"2024-10-29T10:52:36.760024Z","shell.execute_reply":"2024-10-29T10:53:09.311961Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Đã chuyển đổi thành công và lưu vào combined_file_upper.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# Bước 1: Đọc dữ liệu từ file .txt\nwith open('/kaggle/input/trainplusdataset/datasetVN.txt', 'r', encoding='utf-8') as txt_file:\n    data = txt_file.read()  # Đọc toàn bộ nội dung file\n\n# Bước 2: Lưu dữ liệu vào file .pkl\nwith open('/kaggle/working/file.pkl', 'wb') as pkl_file:\n    pickle.dump(data, pkl_file)  # Lưu dữ liệu vào file pickle\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:53:49.995329Z","iopub.execute_input":"2024-10-29T10:53:49.995771Z","iopub.status.idle":"2024-10-29T10:53:50.844046Z","shell.execute_reply.started":"2024-10-29T10:53:49.995732Z","shell.execute_reply":"2024-10-29T10:53:50.843119Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Bước 1: Tải cả bốn tập dữ liệu\nwith open('/kaggle/input/trainplusdataset/combined_file.pkl', 'rb') as file1:\n    combined_data = pickle.load(file1)\n\nwith open('/kaggle/working/combined_file_upper.pkl', 'rb') as file2:\n    detector_data = pickle.load(file2)\n\nwith open('/kaggle/working/file.pkl', 'rb') as file3:\n    locationvn_data = pickle.load(file3)\n\nwith open('/kaggle/input/trainplusdataset/input_cutdown_corpus.pkl', 'rb') as file4:\n    detector2_data = pickle.load(file4)\n    \n# Bước 2: Gộp dữ liệu\nmerged2_data = []\n\n# Kiểm tra kiểu dữ liệu của từng biến và thêm vào merged_data\nfor data in [combined_data, detector_data, locationvn_data, detector2_data]:\n    if isinstance(data, list):\n        merged2_data.extend(data)  # Nếu là danh sách, sử dụng extend để thêm phần tử vào merged_data\n    elif isinstance(data, dict):\n        merged2_data.append(data)  # Nếu là từ điển, thêm nguyên bản vào merged_data\n    elif isinstance(data, str):\n        merged2_data.append(data)  # Nếu là chuỗi, thêm vào như một phần tử\n    else:\n        raise ValueError(f\"Dữ liệu không phải là danh sách, từ điển hoặc chuỗi: {type(data)}\")\n\n# Bước 3: Lưu lại dữ liệu đã gộp\nwith open('/kaggle/working/merged_full_data.pkl', 'wb') as merged2_file:\n    pickle.dump(merged2_data, merged2_file)\n\nprint(\"Đã gộp thành công và lưu vào merged_data.pkl\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:56:01.296175Z","iopub.execute_input":"2024-10-29T10:56:01.296562Z","iopub.status.idle":"2024-10-29T10:56:42.054404Z","shell.execute_reply.started":"2024-10-29T10:56:01.296525Z","shell.execute_reply":"2024-10-29T10:56:42.053499Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Đã gộp thành công và lưu vào merged_data.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pickle\nfrom unidecode import unidecode\nimport itertools\nfrom nltk import ngrams\nfrom tqdm import tqdm\n\npath_corpus = r\"/kaggle/input/trainplusdataset/merged_full_data.pkl\"\n\nwith open(path_corpus, \"rb\") as f:\n    data = pickle.load(f)\n\nalphabet = r'^[ _abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ!\"\\',\\-\\.:;?_\\(\\)]+$'\n\n#Extracting sentence from corpus\ndef latin_extract(data):\n\n    # extract Latin- characters only\n    latin_extract_data=[]\n    # duyet qua tung van ban\n    for i in data:\n      if i == 1:\n        break\n      # thay the xuong dong la dau cham ket thuc\n      i=i.replace(\"\\n\",\".\")\n      # tach van ban theo dau cham ket thuc\n      sentences=i.split(\".\")\n      for j in sentences:\n          if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n\n              latin_extract_data.append(j)\n\n    return latin_extract_data\n\ntraining_data = latin_extract(data)\ni = 100\n# Danh sách các chữ cái tiếng Việt (cả chữ thường và chữ in hoa)\nletters = list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ\" \n               \"ABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n\nletters2 = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\n# Định nghĩa các từ điển lỗi chính tả bao gồm cả chữ thường và chữ in hoa\ntypo = {\n    \"ă\": \"aw\", \"â\": \"aa\", \"á\": \"as\", \"à\": \"af\", \"ả\": \"ar\", \"ã\": \"ax\", \"ạ\": \"aj\",\n    \"ắ\": \"aws\", \"ổ\": \"oor\", \"ỗ\": \"oox\", \"ộ\": \"ooj\", \"ơ\": \"ow\", \"ằ\": \"awf\", \"ẳ\": \"awr\", \n    \"ẵ\": \"awx\", \"ặ\": \"awj\", \"ó\": \"os\", \"ò\": \"of\", \"ỏ\": \"or\", \"õ\": \"ox\", \"ọ\": \"oj\", \n    \"ô\": \"oo\", \"ố\": \"oos\", \"ồ\": \"oof\", \"ớ\": \"ows\", \"ờ\": \"owf\", \"ở\": \"owr\", \"ỡ\": \"owx\", \n    \"ợ\": \"owj\", \"é\": \"es\", \"è\": \"ef\", \"ẻ\": \"er\", \"ẽ\": \"ex\", \"ẹ\": \"ej\", \"ê\": \"ee\", \n    \"ế\": \"ees\", \"ề\": \"eef\", \"ể\": \"eer\", \"ễ\": \"eex\", \"ệ\": \"eej\", \"ú\": \"us\", \"ù\": \"uf\", \n    \"ủ\": \"ur\", \"ũ\": \"ux\", \"ụ\": \"uj\", \"ư\": \"uw\", \"ứ\": \"uws\", \"ừ\": \"uwf\", \"ử\": \"uwr\", \n    \"ữ\": \"uwx\", \"ự\": \"uwj\", \"í\": \"is\", \"ì\": \"if\", \"ỉ\": \"ir\", \"ị\": \"ij\", \"ĩ\": \"ix\", \n    \"ý\": \"ys\", \"ỳ\": \"yf\", \"ỷ\": \"yr\", \"ỵ\": \"yj\", \"đ\": \"dd\",\n    # Chữ in hoa\n    \"Ă\": \"Aw\", \"Â\": \"Aa\", \"Á\": \"As\", \"À\": \"Af\", \"Ả\": \"Ar\", \"Ã\": \"Ax\", \"Ạ\": \"Aj\",\n    \"Ắ\": \"Aws\", \"Ổ\": \"Oor\", \"Ỗ\": \"Oox\", \"Ộ\": \"Ooj\", \"Ơ\": \"Ow\", \"Ằ\": \"AWF\", \"Ẳ\": \"Awr\", \n    \"Ẵ\": \"Awx\", \"Ặ\": \"Awj\", \"Ó\": \"Os\", \"Ò\": \"Of\", \"Ỏ\": \"Or\", \"Õ\": \"Ox\", \"Ọ\": \"Oj\", \n    \"Ô\": \"Oo\", \"Ố\": \"Oos\", \"Ồ\": \"Oof\", \"Ớ\": \"Ows\", \"Ờ\": \"Owf\", \"Ở\": \"Owr\", \"Ỡ\": \"Owx\", \n    \"Ợ\": \"Owj\", \"É\": \"Es\", \"È\": \"Ef\", \"Ẻ\": \"Er\", \"Ẽ\": \"Ex\", \"Ẹ\": \"Ej\", \"Ê\": \"Ee\", \n    \"Ế\": \"Ees\", \"Ề\": \"Eef\", \"Ể\": \"Eer\", \"Ễ\": \"Eex\", \"Ệ\": \"Eej\", \"Ú\": \"Us\", \"Ù\": \"Uf\", \n    \"Ủ\": \"Ur\", \"Ũ\": \"Ux\", \"Ụ\": \"Uj\", \"Ư\": \"Uw\", \"Ứ\": \"Uws\", \"Ừ\": \"Uwf\", \"Ử\": \"Uwr\", \n    \"Ữ\": \"Uwx\", \"Ự\": \"Uwj\", \"Í\": \"Is\", \"Ì\": \"If\", \"Ỉ\": \"Ir\", \"Ị\": \"Ij\", \"Ĩ\": \"Ix\", \n    \"Ý\": \"Ys\", \"Ỳ\": \"Yf\", \"Ỷ\": \"Yr\", \"Ỵ\": \"Yj\", \"Đ\": \"Dd\"\n}\n\n# Lỗi chính tả địa phương\nregion = {\n    \"ẻ\": \"ẽ\", \"ẽ\": \"ẻ\", \"ũ\": \"ủ\", \"ủ\": \"ũ\", \"ã\": \"ả\", \"ả\": \"ã\", \"ỏ\": \"õ\", \"õ\": \"ỏ\", \"i\": \"j\",\n    \"Ẻ\": \"Ẽ\", \"Ẽ\": \"Ẻ\", \"Ũ\": \"Ủ\", \"Ủ\": \"Ũ\", \"Ã\": \"Ả\", \"Ả\": \"Ã\", \"Ỏ\": \"Õ\", \"Õ\": \"Ỏ\", \"I\": \"J\"\n}\n\n# Lỗi chính tả theo vùng miền cho chữ viết hoa và thường\nregion2 = {\n    \"s\": \"x\", \"l\": \"n\", \"n\": \"l\", \"x\": \"s\", \"d\": \"gi\",\n    \"S\": \"X\", \"L\": \"N\", \"N\": \"L\", \"X\": \"S\", \"D\": \"Gi\", \"Gi\": \"D\"\n}\n\n# Nguyên âm (cả chữ thường và in hoa)\nvowel = list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\" \n             \"AEIOUYÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴ\")\n\n# Các từ viết tắt cho chữ thường và chữ in hoa\nacronym = {\n    \"không\": \"ko\", \" anh\": \" a\", \"em\": \"e\", \"biết\": \"bít\", \"giờ\": \"h\", \"gì\": \"j\", \"muốn\": \"mún\", \"học\": \"hok\", \n    \"yêu\": \"iu\", \"chồng\": \"ck\", \"vợ\": \"vk\", \" ông\": \" ô\", \"được\": \"đc\", \"tôi\": \"t\",\n    \"Không\": \"Ko\", \" Anh\": \" A\", \"Em\": \"E\", \"Biết\": \"Bít\", \"Giờ\": \"H\", \"Gì\": \"J\", \"Muốn\": \"Mún\", \"Học\": \"Hok\", \n    \"Yêu\": \"Iu\", \"Chồng\": \"Ck\", \"Vợ\": \"Vk\", \" Ông\": \" Ô\", \"Được\": \"Đc\", \"Tôi\": \"T\"\n}\n\n# Các quy tắc teencode cho chữ thường và chữ in hoa\nteen = {\n    \"ch\": \"ck\", \"ph\": \"f\", \"th\": \"tk\", \"nh\": \"nk\",\n    \"Ch\": \"Ck\", \"Ph\": \"F\", \"Th\": \"Tk\", \"Nh\": \"Nk\"\n}\n\n\n# function for adding mistake( noise)\ndef teen_code(sentence,pivot):\n    random = np.random.uniform(0,1,1)[0]\n    new_sentence=str(sentence)\n    if random>pivot:\n        for word in acronym.keys():\n            if re.search(word, new_sentence):\n                random2 = np.random.uniform(0,1,1)[0]\n                if random2 <0.5:\n                    new_sentence=new_sentence.replace(word,acronym[word])\n        for word in teen.keys():\n            if re.search(word, new_sentence):\n                random3 = np.random.uniform(0,1,1)[0]\n                if random3 <0.05:\n                    new_sentence=new_sentence.replace(word,teen[word])\n        return new_sentence\n    else:\n        return sentence\n\n\ndef add_noise(sentence, pivot1,pivot2):\n    sentence=teen_code(sentence,0.5)\n    noisy_sentence = \"\"\n    i = 0\n    while i < len(sentence):\n        if sentence[i] not in letters:\n            noisy_sentence+=sentence[i]\n        else:\n            random = np.random.uniform(0,1,1)[0]\n            if random < pivot1:\n                noisy_sentence+=(sentence[i])\n            elif random<pivot2:\n                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n                    random2=np.random.uniform(0,1,1)[0]\n                    if random2<=0.4:\n                        noisy_sentence+=typo[sentence[i]]\n                    elif random2<0.8:\n                        noisy_sentence+=region[sentence[i]]\n                    elif random2<0.95 :\n                        noisy_sentence+=unidecode(sentence[i])\n                    else:\n                        noisy_sentence+=sentence[i]\n                elif sentence[i] in typo.keys():\n                    random3=np.random.uniform(0,1,1)[0]\n                    if random3<=0.6:\n                        noisy_sentence+=typo[sentence[i]]\n                    elif random3<0.9 :\n                        noisy_sentence+=unidecode(sentence[i])\n                    else:\n                        noisy_sentence+=sentence[i]\n                elif sentence[i] in region.keys():\n                    random4=np.random.uniform(0,1,1)[0]\n                    if random4<=0.6:\n                        noisy_sentence+=region[sentence[i]]\n                    elif random4<0.85 :\n                        noisy_sentence+=unidecode(sentence[i])\n                    else:\n                        noisy_sentence+=sentence[i]\n                elif i<len(sentence)-1 :\n                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n                        random5=np.random.uniform(0,1,1)[0]\n                        if random5<=0.9:\n                            noisy_sentence+=region2[sentence[i]]\n                        else:\n                            noisy_sentence+=sentence[i]\n                    else:\n                        noisy_sentence+=sentence[i]\n\n            else:\n                new_random = np.random.uniform(0,1,1)[0]\n                if new_random <=0.33:\n                    if i == (len(sentence) - 1):\n                        continue\n                    else:\n                        noisy_sentence+=(sentence[i+1])\n                        noisy_sentence+=(sentence[i])\n                        i += 1\n                elif new_random <= 0.66:\n                    random_letter = np.random.choice(letters2, 1)[0]\n                    noisy_sentence+=random_letter\n                else:\n                    pass\n\n        i += 1\n    return noisy_sentence\n\ndef extract_phrases(text):\n    return re.findall(r'\\w[\\w ]+', text)\n\ndef _extract_phrases(data):\n    phrases = itertools.chain.from_iterable(extract_phrases(text) for text in data)\n    phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n\n    return phrases\n\nphrases = _extract_phrases(training_data)\n\n#Generate Bi-gram\n\n#A Vietnamese word do not contain more than 7 characters, so an bi-gram do not have more than 15 characters\nNGRAM = 2\nMAXLEN = 40\n\ndef gen_ngrams(words, n=2):\n    return ngrams(words.split(), n)\n#Mục đích: Tạo danh sách các bi-grams từ danh sách các cụm từ (phrases) và lọc ra các bi-grams thỏa mãn các điều kiện.\ndef generate_bi_grams(phrases):\n    list_ngrams = []\n    for p in tqdm(phrases):\n\n      # neu khong nham trong bang chu cai thi bo qua\n      if not re.match(alphabet, p.lower()):\n        continue\n\n      # tach p thanh cac bi gram\n      for ngr in gen_ngrams(p, NGRAM):\n        if len(\" \".join(ngr)) < MAXLEN:\n          list_ngrams.append(\" \".join(ngr))\n\n    return list_ngrams\n\nlist_ngrams = generate_bi_grams(phrases)\n\nprint(len(list_ngrams))\n\nalphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n\n#Chuyển đổi một đoạn văn bản thành dạng mã hóa one-hot có độ dài cố định (MAXLEN).\n#Đầu ra: Mảng 2D one-hot encoding của văn bản\ndef encoder_data(text, maxlen=MAXLEN):\n        #print(\"Maxlen\", maxlen)\n        text = \"\\x00\" + text\n        #print(\"text\", text)\n        x = np.zeros((maxlen, len(alphabet)))\n        #print(\"X ban dau\", x)\n        for i, c in enumerate(text[:maxlen]):\n            x[i, alphabet.index(c)] = 1\n        if i < maxlen - 1:\n          for j in range(i+1, maxlen):\n            x[j, 0] = 1\n        return x\n#Mục đích: Giải mã dữ liệu one-hot encoding trở lại thành văn bản.\n#Đầu ra: Văn bản đã được giải mã.\ndef decoder_data(x):\n    x = x.argmax(axis=-1)\n    #print(\"x hien tai\", x)\n    dem = ''.join(alphabet[i] for i in x)\n    #print(\"Do dai cau van\", len(dem))\n\n    return dem\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:14:01.126900Z","iopub.execute_input":"2024-10-29T11:14:01.127331Z","iopub.status.idle":"2024-10-29T11:23:36.158117Z","shell.execute_reply.started":"2024-10-29T11:14:01.127291Z","shell.execute_reply":"2024-10-29T11:23:36.157160Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"100%|██████████| 44289283/44289283 [05:05<00:00, 145069.02it/s]","output_type":"stream"},{"name":"stdout","text":"105824037\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n# import preprocessing\nimport os\n# from preprocessing import MAXLEN, alphabet\n\nencoder = LSTM(256,input_shape=(MAXLEN, len(alphabet)), return_sequences=True)\n\ndecoder = Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))\n\nmodel=Sequential()\nmodel.add(encoder)\nmodel.add(decoder)\nmodel.add(TimeDistributed(Dense(256)))\nmodel.add(Activation('relu'))\nmodel.add(TimeDistributed(Dense(len(alphabet))))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(learning_rate=0.001),\n              metrics=['accuracy'])\n\nmodel.summary()\n\n#Spliting training data\ntrain_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)\n\n# we have to use data- generation medthod cause this dataset is too large to fit into memory\nBATCH_SIZE = 512\ndef generate_data(data, batch_size):\n    cur_index = 0\n    while True:\n        x, y = [], []\n        for i in range(batch_size):\n            y.append(encoder_data(data[cur_index]))\n            x.append(encoder_data(add_noise(data[cur_index],0.94,0.985)))\n            cur_index += 1\n            if cur_index > len(data)-1:\n                cur_index = 0\n        yield np.array(x), np.array(y)\n\ntrain_generator = generate_data(train_data, batch_size=BATCH_SIZE)\nvalidation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T11:23:36.159816Z","iopub.execute_input":"2024-10-29T11:23:36.160165Z","iopub.status.idle":"2024-10-29T11:25:26.319813Z","shell.execute_reply.started":"2024-10-29T11:23:36.160126Z","shell.execute_reply":"2024-10-29T11:25:26.319012Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m466,944\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,050,624\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m131,328\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m199\u001b[0m)        │        \u001b[38;5;34m51,143\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m199\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">466,944</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,143</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,700,039\u001b[0m (6.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,700,039</span> (6.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,700,039\u001b[0m (6.49 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,700,039</span> (6.49 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"\n# train the model and save to the Model folder\n# Use the .keras format for saving\nfrom keras.callbacks import ModelCheckpoint\nimport os\n\n# Cấu hình checkpoint để lưu sau mỗi epoch\ncheckpointer = ModelCheckpoint(\n    filepath=os.path.join('/kaggle/working/spelling_epoch_{epoch:02d}.keras'),\n    save_best_only=False,  # Lưu tất cả các epoch\n    verbose=1,\n    save_freq='epoch'  # Lưu checkpoint sau mỗi epoch\n)\n\n# Huấn luyện mô hình    \nmodel.fit(\n    train_generator, \n    steps_per_epoch=len(train_data) // BATCH_SIZE, \n    epochs=2,\n    validation_data=validation_generator, \n    validation_steps=len(valid_data) // BATCH_SIZE,\n    callbacks=[checkpointer],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
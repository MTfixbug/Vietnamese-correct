{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "1w59ksogVk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FmE4YnxPphoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "Ouq6HjZFW02q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **test1**\n",
        "ê test1 đúng hơn nha"
      ],
      "metadata": {
        "id": "zInV4WiLg1ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Tối ưu TensorFlow để sử dụng GPU trước khi khởi tạo mô hình\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Không thể thiết lập set_memory_growth: {e}\")\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from tensorflow.keras.models import load_model\n",
        "from transformers import pipeline\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "# Tải mô hình đầu tiên và đảm bảo chạy trên GPU nếu có\n",
        "model = load_model('/content/drive/MyDrive/Final/spelling_epoch_03.keras')\n",
        "model.make_predict_function()\n",
        "\n",
        "# Các biến số\n",
        "NGRAM = 2\n",
        "MAXLEN = 40  # Kích thước đầu vào của mô hình\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "accepted_char = list(string.digits + ''.join(alphabet))\n",
        "\n",
        "# Định nghĩa hàm call từ mô hình đầu tiên\n",
        "def call(sentence):\n",
        "    def extract_phrases(text):\n",
        "        pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n",
        "        return re.findall(pattern, text)\n",
        "\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            if c in alphabet:\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "            for j in range(i + 1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "        return x\n",
        "\n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def batch_predict(ngrams_batch):\n",
        "        batch_input = np.array([encoder_data(' '.join(ngram)) for ngram in ngrams_batch])\n",
        "        preds = model.predict(batch_input, verbose=0)\n",
        "        return [decoder_data(pred).strip('\\x00') for pred in preds]\n",
        "\n",
        "    def separate_words(text):\n",
        "        return re.sub(r'(?<!^)(?=[A-Z])', ' ', text)\n",
        "\n",
        "    def correct(sentence):\n",
        "        sentence = separate_words(sentence)\n",
        "\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence = sentence.replace(i, \" \")\n",
        "\n",
        "        ngrams_list = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = batch_predict(ngrams_list)  # Gọi batch_predict thay vì từng n-gram\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in enumerate(guessed_ngrams):\n",
        "            for wid, word in enumerate(re.split(' +', ngram)):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        if not candidates or all(len(c) == 0 for c in candidates):\n",
        "            return \"Không có từ nào để sửa.\"\n",
        "\n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates if c)\n",
        "        return output\n",
        "\n",
        "    return correct(sentence)\n",
        "\n",
        "# Hàm để xử lý đầu vào và đầu ra của Gradio\n",
        "def correct_sentence(sentence):\n",
        "    # Gọi mô hình đầu tiên\n",
        "    first_guess = call(sentence)\n",
        "\n",
        "    # Sử dụng mô hình transformers đã lưu và chạy trên GPU nếu có\n",
        "    corrector = pipeline(\"text2text-generation\", model=\"/content/drive/MyDrive/final2/datasettest1\", tokenizer=\"/content/drive/MyDrive/final2/datasettest1\", device=0)  # device=0 để chạy trên GPU\n",
        "\n",
        "    # Chuyển kết quả từ mô hình đầu tiên sang mô hình transformers\n",
        "    MAX_LENGTH = 512\n",
        "    predictions = corrector(first_guess, max_length=MAX_LENGTH)\n",
        "\n",
        "    # Hiển thị kết quả cuối cùng sau khi mô hình transformers chạy\n",
        "    final_output = predictions[0]['generated_text']\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Tạo giao diện Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=correct_sentence,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Vietnamese Spell Correction\",\n",
        "    description=\"Nhập một câu có thể có lỗi chính tả và nhấn 'Submit' để nhận kết quả sửa lỗi từ mô hình transformers.\"\n",
        ")\n",
        "\n",
        "# Chạy giao diện\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "ZHZQBrGeYW08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "corrector = pipeline(\"text2text-generation\", model=\"bmd1905/vietnamese-correction-v2\")\n"
      ],
      "metadata": {
        "id": "ZfcA6h9OxIgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "texts = [\"CỘNG HÒA XÃ HỘI CHỦ NGỈA VIỆT NAM Độc lập - Tự do - Hạn phúc CĂN CUỚC CÔNG DÂN 030092000101 Họ vầ tên NGYỄN VĂN HẬU Ngày, thág, năm xinh: 12/04/1992 Giới tínhh Nam Quốc tịch Vịêt Nam Quê quán Ngọc Kỳ, Tứ Kì, Hải Đương Nơi thường trú: Ngoc Kỳ, Tứ Kỳ, Hải Đuong Có giá trị đén: 12/04/2032\"\n",
        "]\n",
        "# Batch prediction\n",
        "predictions = corrector(texts, max_length=MAX_LENGTH)\n",
        "\n",
        "# Print predictions\n",
        "for text, pred in zip(texts, predictions):\n",
        "    print(\"- \" + pred['generated_text'])"
      ],
      "metadata": {
        "id": "4fu0QSvyxJEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio_client"
      ],
      "metadata": {
        "id": "ii7_M5Jouo06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "id": "Ks8MEh-RfMVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# test2**"
      ],
      "metadata": {
        "id": "61ETR94zfQGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Tối ưu TensorFlow để sử dụng GPU trước khi khởi tạo mô hình\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Không thể thiết lập set_memory_growth: {e}\")\n",
        "\n",
        "import gradio as gr\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from tensorflow.keras.models import load_model\n",
        "from transformers import pipeline\n",
        "import string\n",
        "\n",
        "# Tải mô hình đầu tiên và đảm bảo chạy trên GPU nếu có\n",
        "model = load_model('/content/drive/MyDrive/Final/spelling_epoch_03.keras')\n",
        "model.make_predict_function()\n",
        "\n",
        "# Các biến số\n",
        "NGRAM = 2\n",
        "MAXLEN = 40  # Kích thước đầu vào của mô hình\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "accepted_char = list(string.digits + ''.join(alphabet))\n",
        "\n",
        "# Định nghĩa hàm call từ mô hình đầu tiên\n",
        "def call(sentence):\n",
        "    def extract_phrases(text):\n",
        "        pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n",
        "        return re.findall(pattern, text)\n",
        "\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            if c in alphabet:\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "            for j in range(i + 1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "        return x\n",
        "\n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def batch_predict(ngrams_batch):\n",
        "        batch_input = np.array([encoder_data(' '.join(ngram)) for ngram in ngrams_batch])\n",
        "        preds = model.predict(batch_input, verbose=0)\n",
        "        return [decoder_data(pred).strip('\\x00') for pred in preds]\n",
        "\n",
        "    def separate_words(text):\n",
        "        return re.sub(r'(?<!^)(?=[A-Z])', ' ', text)\n",
        "\n",
        "    def correct(sentence):\n",
        "        sentence = separate_words(sentence)\n",
        "\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence = sentence.replace(i, \" \")\n",
        "\n",
        "        ngrams_list = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = batch_predict(ngrams_list)\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in enumerate(guessed_ngrams):\n",
        "            for wid, word in enumerate(re.split(' +', ngram)):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        if not candidates or all(len(c) == 0 for c in candidates):\n",
        "            return \"Không có từ nào để sửa.\"\n",
        "\n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates if c)\n",
        "        return output\n",
        "\n",
        "    return correct(sentence)\n",
        "\n",
        "# Hàm để xử lý đầu vào và đầu ra của Gradio\n",
        "def correct_sentence(sentence):\n",
        "    # Chuyển tất cả các ký tự thành chữ thường\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Gọi mô hình đầu tiên\n",
        "    first_guess = call(sentence)\n",
        "\n",
        "    # Sử dụng mô hình transformers đã lưu và chạy trên GPU nếu có\n",
        "    corrector = pipeline(\"text2text-generation\", model=\"/content/drive/MyDrive/final2/datasettest1\", tokenizer=\"/content/drive/MyDrive/final2/datasettest1\", device=0)\n",
        "\n",
        "    # Chuyển kết quả từ mô hình đầu tiên sang mô hình transformers\n",
        "    MAX_LENGTH = 512\n",
        "    predictions = corrector(first_guess, max_length=MAX_LENGTH)\n",
        "\n",
        "    # Hiển thị kết quả cuối cùng sau khi mô hình transformers chạy\n",
        "    final_output = predictions[0]['generated_text']\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Tạo giao diện Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=correct_sentence,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Vietnamese Spell Correction\",\n",
        "    description=\"Nhập một câu có thể có lỗi chính tả và nhấn 'Submit' để nhận kết quả sửa lỗi từ mô hình transformers.\"\n",
        ")\n",
        "\n",
        "# Chạy giao diện\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "tApNFNcQfN5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from tensorflow.keras.models import load_model\n",
        "from transformers import pipeline\n",
        "import string\n",
        "import time\n",
        "\n",
        "# Giải phóng bộ nhớ TensorFlow trước khi tải mô hình\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Chỉ tải mô hình một lần để tiết kiệm tài nguyên\n",
        "model = load_model('/content/drive/MyDrive/Final/my_model_weight.keras')\n",
        "model.make_predict_function()\n",
        "# Các biến số\n",
        "NGRAM = 2\n",
        "MAXLEN = 40  # Kích thước đầu vào của mô hình\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "            'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "            'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',\n",
        "            'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n",
        "            'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n",
        "            'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n",
        "            'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',\n",
        "            'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ',\n",
        "            'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ',\n",
        "            'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ',\n",
        "            'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự',\n",
        "            'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ',\n",
        "            'Đ']\n",
        "accepted_char = list(string.digits + ''.join(alphabet))\n",
        "\n",
        "# Định nghĩa hàm call từ mô hình đầu tiên\n",
        "def call(sentence):\n",
        "    def extract_phrases(text):\n",
        "        pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n",
        "        return re.findall(pattern, text)\n",
        "\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            if c in alphabet:\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "            for j in range(i + 1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "        return x\n",
        "\n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def batch_predict(ngrams_batch):\n",
        "        batch_input = np.array([encoder_data(' '.join(ngram)) for ngram in ngrams_batch])\n",
        "        preds = model.predict(batch_input, verbose=0)\n",
        "        return [decoder_data(pred).strip('\\x00') for pred in preds]\n",
        "\n",
        "    def separate_words(text):\n",
        "        return re.sub(r'(?<!^)(?=[A-Z])', ' ', text)\n",
        "\n",
        "    def correct(sentence):\n",
        "        sentence = separate_words(sentence)\n",
        "\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence = sentence.replace(i, \" \")\n",
        "\n",
        "        ngrams_list = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = batch_predict(ngrams_list)  # Gọi batch_predict thay vì từng n-gram\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in enumerate(guessed_ngrams):\n",
        "            for wid, word in enumerate(re.split(' +', ngram)):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        if not candidates or all(len(c) == 0 for c in candidates):\n",
        "            return \"Không có từ nào để sửa.\"\n",
        "\n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates if c)\n",
        "        return output\n",
        "\n",
        "    return correct(sentence)\n",
        "\n",
        "# Hàm để xử lý đầu vào và đầu ra của Gradio\n",
        "def correct_sentence(sentence):\n",
        "    # Thời gian bắt đầu\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Gọi mô hình đầu tiên\n",
        "    first_guess = call(sentence)\n",
        "\n",
        "    # Sử dụng mô hình transformers đã lưu và chạy trên CPU\n",
        "    corrector = pipeline(\"text2text-generation\", model=\"/content/drive/MyDrive/Final/tanphuc_model\", tokenizer=\"/content/drive/MyDrive/Final/tanphuc_model\", device=-1)  # device=-1 để chạy trên CPU\n",
        "\n",
        "    # Chuyển kết quả từ mô hình đầu tiên sang mô hình transformers\n",
        "    MAX_LENGTH = 512\n",
        "    predictions = corrector(first_guess, max_length=MAX_LENGTH)\n",
        "\n",
        "    # Hiển thị kết quả cuối cùng\n",
        "    final_output = predictions[0]['generated_text']\n",
        "\n",
        "    # Tính thời gian chạy\n",
        "    run_time = time.time() - start_time\n",
        "\n",
        "    # Tính số lượng từ đã sửa và hiển thị từ đã sửa\n",
        "    words_original = sentence.split()\n",
        "    words_corrected = final_output.split()\n",
        "\n",
        "    corrected_words = []\n",
        "    for original, corrected in zip(words_original, words_corrected):\n",
        "        if original != corrected:\n",
        "            corrected_words.append(f\"{original} -> {corrected}\")\n",
        "\n",
        "    num_corrected = len(corrected_words)\n",
        "\n",
        "    return final_output, run_time, num_corrected, corrected_words\n",
        "\n",
        "# Tạo giao diện Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=correct_sentence,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"text\", \"text\", \"text\", \"text\"],  # Thêm đầu ra cho thời gian và số từ đã sửa\n",
        "    title=\"Vietnamese Spell Correction\",\n",
        "    description=\"Nhập một câu có thể có lỗi chính tả và nhấn 'Submit' để nhận kết quả sửa lỗi từ mô hình transformers.\"\n",
        ")\n",
        "\n",
        "# Chạy giao diện\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "AS3SD9iX69G8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}